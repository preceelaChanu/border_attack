{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7deb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already available)\n",
    "# Suppress dependency warnings for pre-installed Kaggle packages\n",
    "!pip install torch torchvision tqdm Pillow numpy --quiet --no-warn-conflicts 2>/dev/null || \\\n",
    " pip install torch torchvision tqdm Pillow numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5924af4",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet statistics\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def normalize(tensor):\n",
    "    \"\"\"Applies ImageNet normalization. Expects tensor in range [0, 1].\"\"\"\n",
    "    mean = torch.tensor(MEAN).view(1, 3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor(STD).view(1, 3, 1, 1).to(tensor.device)\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def create_border_mask(h, w, border_width, device):\n",
    "    \"\"\"Creates a mask that selects only the border pixels.\"\"\"\n",
    "    mask = torch.zeros((1, 3, h, w), device=device)\n",
    "    mask[:, :, :border_width, :] = 1  # top\n",
    "    mask[:, :, h-border_width:, :] = 1  # bottom\n",
    "    mask[:, :, :, :border_width] = 1  # left\n",
    "    mask[:, :, :, w-border_width:] = 1  # right\n",
    "    return mask\n",
    "\n",
    "def stack_borders(image_tensor, border_width):\n",
    "    \"\"\"Extracts borders and stacks them for the fidelity model.\"\"\"\n",
    "    h, w = image_tensor.shape[2], image_tensor.shape[3]\n",
    "    top = image_tensor[:, :, :border_width, :]\n",
    "    bottom = image_tensor[:, :, h-border_width:, :]\n",
    "    left = image_tensor[:, :, border_width:h-border_width, :border_width]\n",
    "    right = image_tensor[:, :, border_width:h-border_width, w-border_width:]\n",
    "    \n",
    "    # Permute side borders to be horizontal strips\n",
    "    left_permuted = left.permute(0, 1, 3, 2)\n",
    "    right_permuted = right.permute(0, 1, 3, 2)\n",
    "    \n",
    "    target_width = image_tensor.shape[3]\n",
    "    \n",
    "    def resize_strip(strip):\n",
    "        return torch.nn.functional.interpolate(\n",
    "            strip, size=(border_width, target_width), \n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "    \n",
    "    stacked = torch.cat([\n",
    "        top, bottom, \n",
    "        resize_strip(left_permuted), \n",
    "        resize_strip(right_permuted)\n",
    "    ], dim=2)\n",
    "    \n",
    "    # Repeat to ensure sufficient height for VGG\n",
    "    while stacked.shape[2] < 64:\n",
    "        stacked = torch.cat([stacked, stacked], dim=2)\n",
    "    \n",
    "    return stacked\n",
    "\n",
    "def truncation_loss(adv_image):\n",
    "    \"\"\"Encourages pixel values to be close to integer values (0-255).\"\"\"\n",
    "    scaled = adv_image * 255.0\n",
    "    target_int = torch.round(scaled).detach()\n",
    "    return torch.abs(target_int/255.0 - adv_image).mean()\n",
    "\n",
    "def load_image(path, size=224):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "def save_image(tensor, path):\n",
    "    \"\"\"Saves a [0, 1] tensor as an image.\"\"\"\n",
    "    tensor = tensor.detach().cpu().squeeze(0)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    img = to_pil(tensor)\n",
    "    img.save(path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155041df",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetModel(nn.Module):\n",
    "    \"\"\"ResNet50 classifier model.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(TargetModel, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True).to(device)\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = normalize(x)\n",
    "        return self.model(x_norm)\n",
    "\n",
    "class Vgg19Fidelity(nn.Module):\n",
    "    \"\"\"VGG19 feature extractor for perceptual loss.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(Vgg19Fidelity, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features.to(device)\n",
    "        self.model = vgg.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        \n",
    "        self.layer_map = {\n",
    "            '0': 'conv_1',\n",
    "            '5': 'conv_2',\n",
    "            '10': 'conv_3',\n",
    "            '19': 'conv_4',\n",
    "            '28': 'conv_5',\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = normalize(x)\n",
    "        features = {}\n",
    "        for name, layer in self.model._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.layer_map:\n",
    "                features[self.layer_map[name]] = x\n",
    "        return features\n",
    "\n",
    "    def compute_loss(self, adv_stacked, clean_stacked):\n",
    "        adv_feats = self(adv_stacked)\n",
    "        clean_feats = self(clean_stacked)\n",
    "        \n",
    "        loss_c = 0.0\n",
    "        loss_s = 0.0\n",
    "        \n",
    "        # Content Loss\n",
    "        for layer in self.content_layers:\n",
    "            loss_c += torch.mean((adv_feats[layer] - clean_feats[layer]) ** 2)\n",
    "        \n",
    "        # Style Loss (Gram Matrix)\n",
    "        for layer in self.style_layers:\n",
    "            a = adv_feats[layer]\n",
    "            c = clean_feats[layer]\n",
    "            \n",
    "            b, ch, h, w = a.shape\n",
    "            a = a.view(b, ch, h * w)\n",
    "            c = c.view(b, ch, h * w)\n",
    "            \n",
    "            gram_a = torch.bmm(a, a.transpose(1, 2)) / (ch * h * w)\n",
    "            gram_c = torch.bmm(c, c.transpose(1, 2)) / (ch * h * w)\n",
    "            \n",
    "            loss_s += torch.mean((gram_a - gram_c) ** 2)\n",
    "        \n",
    "        return loss_c, loss_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f494a",
   "metadata": {},
   "source": [
    "## Targeted Attack Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetedBorderAttack:\n",
    "    \"\"\"Targeted adversarial attack that modifies only image borders.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_model, fidelity_model, device, \n",
    "                 border_width=4, lambda_a=1.0, lambda_f=1000.0, max_iter=50):\n",
    "        self.target_model = target_model\n",
    "        self.fidelity_model = fidelity_model\n",
    "        self.device = device\n",
    "        self.border_width = border_width\n",
    "        self.lambda_a = lambda_a\n",
    "        self.lambda_f = lambda_f\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _targeted_loss(self, logits, target_class):\n",
    "        \"\"\"Cross-entropy loss for targeted attack.\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        target_tensor = torch.tensor([target_class], device=self.device)\n",
    "        return criterion(logits, target_tensor)\n",
    "\n",
    "    def run(self, image, target_class):\n",
    "        \"\"\"\n",
    "        Performs targeted attack.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor (1, 3, H, W) in range [0, 1]\n",
    "            target_class: Target class index to force prediction to\n",
    "            \n",
    "        Returns:\n",
    "            adv_image: Adversarial image\n",
    "            success: Whether attack succeeded\n",
    "        \"\"\"\n",
    "        b, c, h, w = image.shape\n",
    "        mask = create_border_mask(h, w, self.border_width, self.device)\n",
    "        inverse_mask = 1 - mask\n",
    "        \n",
    "        # Initialize adversarial image\n",
    "        adv_image = image.clone().detach().requires_grad_(True)\n",
    "        optimizer = optim.LBFGS([adv_image], max_iter=20, history_size=10)\n",
    "        \n",
    "        clean_stacked_borders = stack_borders(image.detach(), self.border_width)\n",
    "        \n",
    "        iteration = 0\n",
    "        success = False\n",
    "        \n",
    "        print(f\"Running targeted attack to class {target_class}...\")\n",
    "        \n",
    "        while iteration < self.max_iter:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Enforce constraints\n",
    "                with torch.no_grad():\n",
    "                    adv_image.clamp_(0, 1)\n",
    "                    adv_image.data = adv_image.data * mask + image.data * inverse_mask\n",
    "                \n",
    "                logits = self.target_model(adv_image)\n",
    "                \n",
    "                # Attack Loss (targeted)\n",
    "                l_attack = self._targeted_loss(logits, target_class)\n",
    "                \n",
    "                # Fidelity Loss\n",
    "                adv_stacked = stack_borders(adv_image, self.border_width)\n",
    "                l_c, l_s = self.fidelity_model.compute_loss(adv_stacked, clean_stacked_borders)\n",
    "                l_fidelity = l_c + l_s\n",
    "                \n",
    "                # Truncation Loss\n",
    "                l_trunc = truncation_loss(adv_image * mask)\n",
    "                \n",
    "                # Total Loss\n",
    "                total_loss = (self.lambda_a * l_attack) + (self.lambda_f * l_fidelity) + l_trunc\n",
    "                \n",
    "                if adv_image.grad is not None:\n",
    "                    adv_image.grad.data.mul_(mask)\n",
    "                \n",
    "                total_loss.backward()\n",
    "                adv_image.grad.data.mul_(mask)\n",
    "                \n",
    "                return total_loss\n",
    "            \n",
    "            optimizer.step(closure)\n",
    "            \n",
    "            # Check success\n",
    "            with torch.no_grad():\n",
    "                logits = self.target_model(adv_image)\n",
    "                pred = torch.argmax(logits, dim=1).item()\n",
    "                \n",
    "                if pred == target_class:\n",
    "                    success = True\n",
    "                    print(f\"✓ Attack succeeded at iteration {iteration}!\")\n",
    "                    break\n",
    "            \n",
    "            iteration += 1\n",
    "            if iteration % 10 == 0:\n",
    "                print(f\"Iter {iteration}: Prediction {pred}\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        with torch.no_grad():\n",
    "            adv_image.clamp_(0, 1)\n",
    "            adv_image.data = adv_image.data * mask + image.data * inverse_mask\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"✗ Attack failed after {self.max_iter} iterations\")\n",
    "        \n",
    "        return adv_image, success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d777df",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ec3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models...\")\n",
    "target_model = TargetModel(device)\n",
    "fidelity_model = Vgg19Fidelity(device)\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe98522",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n",
    "\n",
    "Upload your test images to Kaggle and update the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437044f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMAGE_DIR = '/kaggle/input/imagenet-validation-set/processed/processed'  # UPDATE THIS PATH\n",
    "TARGET_CLASS = 207  # Example: Golden Retriever in ImageNet\n",
    "BORDER_WIDTH = 4\n",
    "MAX_ITERATIONS = 50\n",
    "OUTPUT_DIR = '/kaggle/working/results'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ImageNet class names (first few examples)\n",
    "imagenet_classes = {\n",
    "    0: 'tench', 1: 'goldfish', 2: 'great white shark',\n",
    "    207: 'golden retriever', 208: 'Labrador retriever',\n",
    "    281: 'tabby cat', 282: 'tiger cat',\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded.\")\n",
    "print(f\"Image directory: {IMAGE_DIR}\")\n",
    "print(f\"Target class: {TARGET_CLASS} ({imagenet_classes.get(TARGET_CLASS, 'Unknown')})\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6af2c9",
   "metadata": {},
   "source": [
    "## Run Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Attack on All Images\n",
    "import glob\n",
    "\n",
    "print(f\"Scanning directory: {IMAGE_DIR}\")\n",
    "# Support multiple image formats including .pt tensor files\n",
    "image_paths = []\n",
    "for ext in ['*.pt', '*.jpg', '*.jpeg', '*.png', '*.JPEG', '*.JPG', '*.PNG']:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_DIR, ext)))\n",
    "\n",
    "print(f\"Found {len(image_paths)} images to attack\")\n",
    "\n",
    "# Initialize attacker\n",
    "attacker = TargetedBorderAttack(\n",
    "    target_model, \n",
    "    fidelity_model, \n",
    "    device,\n",
    "    border_width=BORDER_WIDTH,\n",
    "    max_iter=MAX_ITERATIONS,\n",
    "    lambda_a=1.0,  # For targeted attacks\n",
    "    lambda_f=1000.0\n",
    ")\n",
    "\n",
    "results = []\n",
    "success_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for i, img_path in enumerate(tqdm(image_paths, desc=\"Attacking images\")):\n",
    "    try:\n",
    "        # Load image (handle both .pt files and regular images)\n",
    "        if img_path.endswith('.pt'):\n",
    "            image = torch.load(img_path, map_location=device)\n",
    "            # Ensure correct shape (1, 3, H, W)\n",
    "            if image.dim() == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "        else:\n",
    "            image = load_image(img_path).to(device)\n",
    "        \n",
    "        # Get original prediction\n",
    "        with torch.no_grad():\n",
    "            orig_logits = target_model(image)\n",
    "            orig_pred = torch.argmax(orig_logits, dim=1).item()\n",
    "            orig_conf = torch.softmax(orig_logits, dim=1)[0, orig_pred].item()\n",
    "        \n",
    "        # Skip if already the target class\n",
    "        if orig_pred == TARGET_CLASS:\n",
    "            skipped_count += 1\n",
    "            results.append({\n",
    "                'filename': os.path.basename(img_path),\n",
    "                'original_pred': orig_pred,\n",
    "                'original_conf': orig_conf,\n",
    "                'adversarial_pred': orig_pred,\n",
    "                'adversarial_conf': orig_conf,\n",
    "                'success': True,\n",
    "                'skipped': True\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Run attack\n",
    "        adv_image, success = attacker.run(image, TARGET_CLASS)\n",
    "        \n",
    "        # Get adversarial prediction\n",
    "        with torch.no_grad():\n",
    "            adv_logits = target_model(adv_image)\n",
    "            adv_pred = torch.argmax(adv_logits, dim=1).item()\n",
    "            adv_conf = torch.softmax(adv_logits, dim=1)[0, adv_pred].item()\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            'filename': os.path.basename(img_path),\n",
    "            'original_pred': orig_pred,\n",
    "            'original_conf': orig_conf,\n",
    "            'adversarial_pred': adv_pred,\n",
    "            'adversarial_conf': adv_conf,\n",
    "            'success': success,\n",
    "            'skipped': False\n",
    "        })\n",
    "        \n",
    "        # Save adversarial image\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        save_path = os.path.join(OUTPUT_DIR, f\"adv_{base_name}.png\")\n",
    "        save_image(adv_image, save_path)\n",
    "        \n",
    "        # Print progress every 100 images\n",
    "        if (i + 1) % 100 == 0:\n",
    "            current_asr = (success_count / (i + 1 - skipped_count)) * 100 if (i + 1 - skipped_count) > 0 else 0\n",
    "            print(f\"\\nProgress: {i+1}/{len(image_paths)} | Current ASR: {current_asr:.2f}% | Skipped: {skipped_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {img_path}: {e}\")\n",
    "        results.append({\n",
    "            'filename': os.path.basename(img_path),\n",
    "            'original_pred': -1,\n",
    "            'original_conf': 0.0,\n",
    "            'adversarial_pred': -1,\n",
    "            'adversarial_conf': 0.0,\n",
    "            'success': False,\n",
    "            'skipped': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Calculate final statistics\n",
    "total_attempted = len([r for r in results if not r.get('skipped', False) and 'error' not in r])\n",
    "final_asr = (success_count / total_attempted * 100) if total_attempted > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ATTACK SUMMARY (Targeted to class {TARGET_CLASS})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total images found: {len(image_paths)}\")\n",
    "print(f\"Skipped (already target class): {skipped_count}\")\n",
    "print(f\"Images attacked: {total_attempted}\")\n",
    "print(f\"Successful attacks: {success_count}\")\n",
    "print(f\"Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd62b9",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03321020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sample Results (first successful attack)\n",
    "successful_results = [r for r in results if r.get('success', False) and not r.get('skipped', False)]\n",
    "\n",
    "if successful_results:\n",
    "    # Get first successful attack\n",
    "    sample = successful_results[0]\n",
    "    sample_path = os.path.join(IMAGE_DIR, sample['filename'])\n",
    "    \n",
    "    # Load original image (handle .pt files)\n",
    "    if sample_path.endswith('.pt'):\n",
    "        orig_image = torch.load(sample_path, map_location='cpu')\n",
    "        if orig_image.dim() == 3:\n",
    "            orig_image = orig_image.unsqueeze(0)\n",
    "    else:\n",
    "        orig_image = load_image(sample_path)\n",
    "    \n",
    "    adv_path = os.path.join(OUTPUT_DIR, f\"adv_{os.path.splitext(sample['filename'])[0]}.png\")\n",
    "    \n",
    "    orig_img = save_image(orig_image, os.path.join(OUTPUT_DIR, 'sample_original.png'))\n",
    "    adv_img = Image.open(adv_path)\n",
    "    \n",
    "    # Compute difference\n",
    "    adv_tensor = load_image(adv_path)\n",
    "    diff = torch.abs(adv_tensor - orig_image)\n",
    "    diff_amplified = diff * 10\n",
    "    diff_img = save_image(diff_amplified, os.path.join(OUTPUT_DIR, 'sample_difference.png'))\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title(f'Original\\nPred: {sample[\"original_pred\"]} ({sample[\"original_conf\"]:.2f})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(adv_img)\n",
    "    axes[1].set_title(f'Adversarial\\nPred: {sample[\"adversarial_pred\"]} ({sample[\"adversarial_conf\"]:.2f})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(diff_img)\n",
    "    axes[2].set_title('Difference (10x amplified)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Sample visualization from: {sample['filename']}\")\n",
    "else:\n",
    "    print(\"No successful attacks to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ff33c",
   "metadata": {},
   "source": [
    "## Batch Processing (Optional)\n",
    "\n",
    "Attack multiple images at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Detailed Results to CSV\n",
    "import csv\n",
    "\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'attack_results.csv')\n",
    "\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    if results:\n",
    "        fieldnames = list(results[0].keys())\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "print(f\"Detailed results saved to: {csv_path}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful_results = [r for r in results if r.get('success', False) and not r.get('skipped', False)]\n",
    "failed_results = [r for r in results if not r.get('success', False) and 'error' not in r and not r.get('skipped', False)]\n",
    "skipped_results = [r for r in results if r.get('skipped', False)]\n",
    "\n",
    "if successful_results:\n",
    "    avg_orig_conf = sum(r['original_conf'] for r in successful_results) / len(successful_results)\n",
    "    avg_adv_conf = sum(r['adversarial_conf'] for r in successful_results) / len(successful_results)\n",
    "    print(f\"Successful Attacks: {len(successful_results)}\")\n",
    "    print(f\"  - Avg Original Confidence: {avg_orig_conf:.4f}\")\n",
    "    print(f\"  - Avg Adversarial Confidence: {avg_adv_conf:.4f}\")\n",
    "\n",
    "if skipped_results:\n",
    "    print(f\"\\nSkipped (already target class): {len(skipped_results)}\")\n",
    "\n",
    "if failed_results:\n",
    "    print(f\"\\nFailed Attacks: {len(failed_results)}\")\n",
    "\n",
    "errors = [r for r in results if 'error' in r]\n",
    "if errors:\n",
    "    print(f\"Errors: {len(errors)}\")\n",
    "    print(\"\\nFirst 5 errors:\")\n",
    "    for r in errors[:5]:\n",
    "        print(f\"  - {r['filename']}: {r.get('error', 'Unknown')}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
