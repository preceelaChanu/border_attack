{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already available)\n",
    "# Suppress dependency warnings for pre-installed Kaggle packages\n",
    "!pip install torch torchvision tqdm Pillow numpy --quiet --no-warn-conflicts 2>/dev/null || \\\n",
    " pip install torch torchvision tqdm Pillow numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3fa54",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac98ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet statistics\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def normalize(tensor):\n",
    "    \"\"\"Applies ImageNet normalization. Expects tensor in range [0, 1].\"\"\"\n",
    "    mean = torch.tensor(MEAN).view(1, 3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor(STD).view(1, 3, 1, 1).to(tensor.device)\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def create_border_mask(h, w, border_width, device):\n",
    "    \"\"\"Creates a mask that selects only the border pixels.\"\"\"\n",
    "    mask = torch.zeros((1, 3, h, w), device=device)\n",
    "    mask[:, :, :border_width, :] = 1  # top\n",
    "    mask[:, :, h-border_width:, :] = 1  # bottom\n",
    "    mask[:, :, :, :border_width] = 1  # left\n",
    "    mask[:, :, :, w-border_width:] = 1  # right\n",
    "    return mask\n",
    "\n",
    "def stack_borders(image_tensor, border_width):\n",
    "    \"\"\"Extracts borders and stacks them for the fidelity model.\"\"\"\n",
    "    h, w = image_tensor.shape[2], image_tensor.shape[3]\n",
    "    top = image_tensor[:, :, :border_width, :]\n",
    "    bottom = image_tensor[:, :, h-border_width:, :]\n",
    "    left = image_tensor[:, :, border_width:h-border_width, :border_width]\n",
    "    right = image_tensor[:, :, border_width:h-border_width, w-border_width:]\n",
    "    \n",
    "    # Permute side borders to be horizontal strips\n",
    "    left_permuted = left.permute(0, 1, 3, 2)\n",
    "    right_permuted = right.permute(0, 1, 3, 2)\n",
    "    \n",
    "    target_width = image_tensor.shape[3]\n",
    "    \n",
    "    def resize_strip(strip):\n",
    "        return torch.nn.functional.interpolate(\n",
    "            strip, size=(border_width, target_width), \n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "    \n",
    "    stacked = torch.cat([\n",
    "        top, bottom, \n",
    "        resize_strip(left_permuted), \n",
    "        resize_strip(right_permuted)\n",
    "    ], dim=2)\n",
    "    \n",
    "    # Repeat to ensure sufficient height for VGG\n",
    "    while stacked.shape[2] < 64:\n",
    "        stacked = torch.cat([stacked, stacked], dim=2)\n",
    "    \n",
    "    return stacked\n",
    "\n",
    "def truncation_loss(adv_image):\n",
    "    \"\"\"Encourages pixel values to be close to integer values (0-255).\"\"\"\n",
    "    scaled = adv_image * 255.0\n",
    "    target_int = torch.round(scaled).detach()\n",
    "    return torch.abs(target_int/255.0 - adv_image).mean()\n",
    "\n",
    "def load_image(path, size=224):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "def save_image(tensor, path):\n",
    "    \"\"\"Saves a [0, 1] tensor as an image.\"\"\"\n",
    "    tensor = tensor.detach().cpu().squeeze(0)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    img = to_pil(tensor)\n",
    "    img.save(path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f639248",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetModel(nn.Module):\n",
    "    \"\"\"ResNet50 classifier model.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(TargetModel, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True).to(device)\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = normalize(x)\n",
    "        return self.model(x_norm)\n",
    "\n",
    "class Vgg19Fidelity(nn.Module):\n",
    "    \"\"\"VGG19 feature extractor for perceptual loss.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(Vgg19Fidelity, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features.to(device)\n",
    "        self.model = vgg.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        \n",
    "        self.layer_map = {\n",
    "            '0': 'conv_1',\n",
    "            '5': 'conv_2',\n",
    "            '10': 'conv_3',\n",
    "            '19': 'conv_4',\n",
    "            '28': 'conv_5',\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = normalize(x)\n",
    "        features = {}\n",
    "        for name, layer in self.model._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.layer_map:\n",
    "                features[self.layer_map[name]] = x\n",
    "        return features\n",
    "\n",
    "    def compute_loss(self, adv_stacked, clean_stacked):\n",
    "        adv_feats = self(adv_stacked)\n",
    "        clean_feats = self(clean_stacked)\n",
    "        \n",
    "        loss_c = 0.0\n",
    "        loss_s = 0.0\n",
    "        \n",
    "        # Content Loss\n",
    "        for layer in self.content_layers:\n",
    "            loss_c += torch.mean((adv_feats[layer] - clean_feats[layer]) ** 2)\n",
    "        \n",
    "        # Style Loss (Gram Matrix)\n",
    "        for layer in self.style_layers:\n",
    "            a = adv_feats[layer]\n",
    "            c = clean_feats[layer]\n",
    "            \n",
    "            b, ch, h, w = a.shape\n",
    "            a = a.view(b, ch, h * w)\n",
    "            c = c.view(b, ch, h * w)\n",
    "            \n",
    "            gram_a = torch.bmm(a, a.transpose(1, 2)) / (ch * h * w)\n",
    "            gram_c = torch.bmm(c, c.transpose(1, 2)) / (ch * h * w)\n",
    "            \n",
    "            loss_s += torch.mean((gram_a - gram_c) ** 2)\n",
    "        \n",
    "        return loss_c, loss_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91767581",
   "metadata": {},
   "source": [
    "## Untargeted Attack Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UntargetedBorderAttack:\n",
    "    \"\"\"Untargeted adversarial attack that modifies only image borders.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_model, fidelity_model, device, \n",
    "                 border_width=4, lambda_a=1.5, lambda_f=1000.0, max_iter=50):\n",
    "        self.target_model = target_model\n",
    "        self.fidelity_model = fidelity_model\n",
    "        self.device = device\n",
    "        self.border_width = border_width\n",
    "        self.lambda_a = lambda_a\n",
    "        self.lambda_f = lambda_f\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _untargeted_loss(self, logits, original_class):\n",
    "        \"\"\"\n",
    "        Margin loss for untargeted attack.\n",
    "        L_margin = Z(x')_y - max(Z(x')_i) for i != y\n",
    "        We minimize this to reduce the score of true class and increase others.\n",
    "        \"\"\"\n",
    "        z_y = logits[0, original_class]\n",
    "        \n",
    "        # Get max of other classes\n",
    "        logits_others = logits.clone()\n",
    "        logits_others[0, original_class] = -float('inf')\n",
    "        z_max_other, _ = torch.max(logits_others, dim=1)\n",
    "        \n",
    "        l_margin = z_y - z_max_other\n",
    "        \n",
    "        # Leaky ReLU: max(L_margin, 0.1 * L_margin)\n",
    "        l_a = torch.max(l_margin, 0.1 * l_margin)\n",
    "        return l_a\n",
    "\n",
    "    def run(self, image, original_class):\n",
    "        \"\"\"\n",
    "        Performs untargeted attack.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor (1, 3, H, W) in range [0, 1]\n",
    "            original_class: Ground truth class to avoid\n",
    "            \n",
    "        Returns:\n",
    "            adv_image: Adversarial image\n",
    "            success: Whether attack succeeded (misclassified)\n",
    "        \"\"\"\n",
    "        b, c, h, w = image.shape\n",
    "        mask = create_border_mask(h, w, self.border_width, self.device)\n",
    "        inverse_mask = 1 - mask\n",
    "        \n",
    "        # Initialize adversarial image\n",
    "        adv_image = image.clone().detach().requires_grad_(True)\n",
    "        optimizer = optim.LBFGS([adv_image], max_iter=20, history_size=10)\n",
    "        \n",
    "        clean_stacked_borders = stack_borders(image.detach(), self.border_width)\n",
    "        \n",
    "        iteration = 0\n",
    "        success = False\n",
    "        \n",
    "        print(f\"Running untargeted attack (original class: {original_class})...\")\n",
    "        \n",
    "        while iteration < self.max_iter:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Enforce constraints\n",
    "                with torch.no_grad():\n",
    "                    adv_image.clamp_(0, 1)\n",
    "                    adv_image.data = adv_image.data * mask + image.data * inverse_mask\n",
    "                \n",
    "                logits = self.target_model(adv_image)\n",
    "                \n",
    "                # Attack Loss (untargeted)\n",
    "                l_attack = self._untargeted_loss(logits, original_class)\n",
    "                \n",
    "                # Fidelity Loss\n",
    "                adv_stacked = stack_borders(adv_image, self.border_width)\n",
    "                l_c, l_s = self.fidelity_model.compute_loss(adv_stacked, clean_stacked_borders)\n",
    "                l_fidelity = l_c + l_s\n",
    "                \n",
    "                # Truncation Loss\n",
    "                l_trunc = truncation_loss(adv_image * mask)\n",
    "                \n",
    "                # Total Loss\n",
    "                total_loss = (self.lambda_a * l_attack) + (self.lambda_f * l_fidelity) + l_trunc\n",
    "                \n",
    "                if adv_image.grad is not None:\n",
    "                    adv_image.grad.data.mul_(mask)\n",
    "                \n",
    "                total_loss.backward()\n",
    "                adv_image.grad.data.mul_(mask)\n",
    "                \n",
    "                return total_loss\n",
    "            \n",
    "            optimizer.step(closure)\n",
    "            \n",
    "            # Check success\n",
    "            with torch.no_grad():\n",
    "                logits = self.target_model(adv_image)\n",
    "                pred = torch.argmax(logits, dim=1).item()\n",
    "                \n",
    "                if pred != original_class:  # Misclassified = success\n",
    "                    success = True\n",
    "                    print(f\"✓ Attack succeeded at iteration {iteration}! Misclassified to class {pred}\")\n",
    "                    break\n",
    "            \n",
    "            iteration += 1\n",
    "            if iteration % 10 == 0:\n",
    "                print(f\"Iter {iteration}: Prediction {pred}\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        with torch.no_grad():\n",
    "            adv_image.clamp_(0, 1)\n",
    "            adv_image.data = adv_image.data * mask + image.data * inverse_mask\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"✗ Attack failed after {self.max_iter} iterations\")\n",
    "        \n",
    "        return adv_image, success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a6391",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models...\")\n",
    "target_model = TargetModel(device)\n",
    "fidelity_model = Vgg19Fidelity(device)\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7827f50",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n",
    "\n",
    "Upload your test images to Kaggle and update the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa805eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMAGE_DIR = '/kaggle/input/imagenet-validation-set/processed/processed'  # UPDATE THIS PATH\n",
    "BORDER_WIDTH = 4\n",
    "MAX_ITERATIONS = 50\n",
    "OUTPUT_DIR = '/kaggle/working/results'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded.\")\n",
    "print(f\"Image directory: {IMAGE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfbfd5",
   "metadata": {},
   "source": [
    "## Run Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Attack on All Images\n",
    "import glob\n",
    "\n",
    "print(f\"Scanning directory: {IMAGE_DIR}\")\n",
    "# Support multiple image formats including .pt tensor files\n",
    "image_paths = []\n",
    "for ext in ['*.pt', '*.jpg', '*.jpeg', '*.png', '*.JPEG', '*.JPG', '*.PNG']:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_DIR, ext)))\n",
    "\n",
    "print(f\"Found {len(image_paths)} images to attack\")\n",
    "\n",
    "# Initialize attacker\n",
    "attacker = UntargetedBorderAttack(\n",
    "    target_model, \n",
    "    fidelity_model, \n",
    "    device,\n",
    "    border_width=BORDER_WIDTH,\n",
    "    max_iter=MAX_ITERATIONS,\n",
    "    lambda_a=1.5,  # Higher for untargeted attacks\n",
    "    lambda_f=1000.0\n",
    ")\n",
    "\n",
    "results = []\n",
    "success_count = 0\n",
    "\n",
    "for i, img_path in enumerate(tqdm(image_paths, desc=\"Attacking images\")):\n",
    "    try:\n",
    "        # Load image (handle both .pt files and regular images)\n",
    "        if img_path.endswith('.pt'):\n",
    "            image = torch.load(img_path, map_location=device)\n",
    "            # Ensure correct shape (1, 3, H, W)\n",
    "            if image.dim() == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "        else:\n",
    "            image = load_image(img_path).to(device)\n",
    "        \n",
    "        # Get original prediction\n",
    "        with torch.no_grad():\n",
    "            orig_logits = target_model(image)\n",
    "            orig_pred = torch.argmax(orig_logits, dim=1).item()\n",
    "            orig_conf = torch.softmax(orig_logits, dim=1)[0, orig_pred].item()\n",
    "        \n",
    "        # Run attack\n",
    "        adv_image, success = attacker.run(image, orig_pred)\n",
    "        \n",
    "        # Get adversarial prediction\n",
    "        with torch.no_grad():\n",
    "            adv_logits = target_model(adv_image)\n",
    "            adv_pred = torch.argmax(adv_logits, dim=1).item()\n",
    "            adv_conf = torch.softmax(adv_logits, dim=1)[0, adv_pred].item()\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            'filename': os.path.basename(img_path),\n",
    "            'original_pred': orig_pred,\n",
    "            'original_conf': orig_conf,\n",
    "            'adversarial_pred': adv_pred,\n",
    "            'adversarial_conf': adv_conf,\n",
    "            'success': success\n",
    "        })\n",
    "        \n",
    "        # Save adversarial image\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        save_path = os.path.join(OUTPUT_DIR, f\"adv_{base_name}.png\")\n",
    "        save_image(adv_image, save_path)\n",
    "        \n",
    "        # Print progress every 100 images\n",
    "        if (i + 1) % 100 == 0:\n",
    "            current_asr = (success_count / (i + 1)) * 100\n",
    "            print(f\"\\nProgress: {i+1}/{len(image_paths)} | Current ASR: {current_asr:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {img_path}: {e}\")\n",
    "        results.append({\n",
    "            'filename': os.path.basename(img_path),\n",
    "            'original_pred': -1,\n",
    "            'original_conf': 0.0,\n",
    "            'adversarial_pred': -1,\n",
    "            'adversarial_conf': 0.0,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Calculate final statistics\n",
    "total_attacked = len([r for r in results if 'error' not in r])\n",
    "final_asr = (success_count / total_attacked * 100) if total_attacked > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ATTACK SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total images found: {len(image_paths)}\")\n",
    "print(f\"Successfully attacked: {total_attacked}\")\n",
    "print(f\"Successful attacks: {success_count}\")\n",
    "print(f\"Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c863d",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sample Results (first successful attack)\n",
    "successful_results = [r for r in results if r.get('success', False)]\n",
    "\n",
    "if successful_results:\n",
    "    # Get first successful attack\n",
    "    sample = successful_results[0]\n",
    "    sample_path = os.path.join(IMAGE_DIR, sample['filename'])\n",
    "    \n",
    "    # Load original image (handle .pt files)\n",
    "    if sample_path.endswith('.pt'):\n",
    "        orig_image = torch.load(sample_path, map_location='cpu')\n",
    "        if orig_image.dim() == 3:\n",
    "            orig_image = orig_image.unsqueeze(0)\n",
    "    else:\n",
    "        orig_image = load_image(sample_path)\n",
    "    \n",
    "    adv_path = os.path.join(OUTPUT_DIR, f\"adv_{os.path.splitext(sample['filename'])[0]}.png\")\n",
    "    \n",
    "    orig_img = save_image(orig_image, os.path.join(OUTPUT_DIR, 'sample_original.png'))\n",
    "    adv_img = Image.open(adv_path)\n",
    "    \n",
    "    # Compute difference\n",
    "    adv_tensor = load_image(adv_path)\n",
    "    diff = torch.abs(adv_tensor - orig_image)\n",
    "    diff_amplified = diff * 10\n",
    "    diff_img = save_image(diff_amplified, os.path.join(OUTPUT_DIR, 'sample_difference.png'))\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title(f'Original\\nPred: {sample[\"original_pred\"]} ({sample[\"original_conf\"]:.2f})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(adv_img)\n",
    "    axes[1].set_title(f'Adversarial\\nPred: {sample[\"adversarial_pred\"]} ({sample[\"adversarial_conf\"]:.2f})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(diff_img)\n",
    "    axes[2].set_title('Difference (10x amplified)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Sample visualization from: {sample['filename']}\")\n",
    "else:\n",
    "    print(\"No successful attacks to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971279d1",
   "metadata": {},
   "source": [
    "## Batch Processing (Optional)\n",
    "\n",
    "Attack multiple images at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Detailed Results to CSV\n",
    "import csv\n",
    "\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'attack_results.csv')\n",
    "\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    if results:\n",
    "        fieldnames = list(results[0].keys())\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "print(f\"Detailed results saved to: {csv_path}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful_results = [r for r in results if r.get('success', False)]\n",
    "failed_results = [r for r in results if not r.get('success', False) and 'error' not in r]\n",
    "\n",
    "if successful_results:\n",
    "    avg_orig_conf = sum(r['original_conf'] for r in successful_results) / len(successful_results)\n",
    "    avg_adv_conf = sum(r['adversarial_conf'] for r in successful_results) / len(successful_results)\n",
    "    print(f\"Successful Attacks: {len(successful_results)}\")\n",
    "    print(f\"  - Avg Original Confidence: {avg_orig_conf:.4f}\")\n",
    "    print(f\"  - Avg Adversarial Confidence: {avg_adv_conf:.4f}\")\n",
    "\n",
    "if failed_results:\n",
    "    print(f\"\\nFailed Attacks: {len(failed_results)}\")\n",
    "\n",
    "errors = [r for r in results if 'error' in r]\n",
    "if errors:\n",
    "    print(f\"Errors: {len(errors)}\")\n",
    "    print(\"\\nFirst 5 errors:\")\n",
    "    for r in errors[:5]:\n",
    "        print(f\"  - {r['filename']}: {r.get('error', 'Unknown')}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
